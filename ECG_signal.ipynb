{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "36aa567d",
      "metadata": {
        "id": "36aa567d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.io import arff\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib\n",
        "matplotlib.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "plt.style.use(\"ggplot\")\n",
        "import tensorflow as tf\n",
        "from tensorflow import data\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import mae\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score, classification_report\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "MASrpdT8lAc9",
      "metadata": {
        "id": "MASrpdT8lAc9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from scipy.signal import medfilt, butter, filtfilt\n",
        "import pywt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.signal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Reshape\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e432cef",
      "metadata": {
        "id": "9e432cef"
      },
      "source": [
        "# NB: in this work the appropriete metric is Recall : ensure that all anomalies are detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d04b075",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "2d04b075",
        "outputId": "86834c60-8e5d-4dad-ef9a-68aebb03e7d4"
      },
      "outputs": [],
      "source": [
        "normal_df = pd.read_csv(\"/content/ptbdb_normal.csv\")\n",
        "anomaly_df = pd.read_csv(\"/content/ptbdb_abnormal.csv\")\n",
        "normal_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484eea31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "484eea31",
        "outputId": "63c98fcf-845d-4eaf-e579-623923619fc9"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of Normal data\", normal_df.shape)\n",
        "print(\"Shape of Abnormal data\", anomaly_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28dbab8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f28dbab8",
        "outputId": "29d33af0-b371-4195-902b-10b949b3c342"
      },
      "outputs": [],
      "source": [
        "CLASS_NAMES = [\"Normal\", \"Anomaly\"]\n",
        "\n",
        "normal_df_copy = normal_df.copy()\n",
        "anomaly_df_copy = anomaly_df.copy()\n",
        "print(anomaly_df_copy.columns.equals(normal_df_copy.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "56541452",
      "metadata": {
        "id": "56541452"
      },
      "outputs": [],
      "source": [
        "normal_df_copy = normal_df_copy.set_axis(range(1, 189), axis=1)\n",
        "anomaly_df_copy = anomaly_df_copy.set_axis(range(1, 189), axis=1)\n",
        "normal_df_copy = normal_df_copy.assign(target = CLASS_NAMES[0])\n",
        "anomaly_df_copy = anomaly_df_copy.assign(target = CLASS_NAMES[1])\n",
        "\n",
        "\n",
        "df = pd.concat((normal_df_copy, anomaly_df_copy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59098e51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59098e51",
        "outputId": "121d2a6c-0d4f-4e48-f578-fd621c0b348b"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "3415d69d",
      "metadata": {
        "id": "3415d69d"
      },
      "outputs": [],
      "source": [
        "# Extract the first 5 lines from each dataset this is eserved for the inference\n",
        "presentation_normal_data = normal_df.head(5)\n",
        "presentation_anomaly_data = anomaly_df.head(5)\n",
        "\n",
        "normal_df = normal_df.drop(normal_df.index[:5])\n",
        "anomaly_df = anomaly_df.drop(anomaly_df.index[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b4449d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35b4449d",
        "outputId": "0b5e3aff-83ea-4496-a30b-d3c689dce856"
      },
      "outputs": [],
      "source": [
        "normal_df.drop(\"target\", axis=1, errors=\"ignore\", inplace=True)\n",
        "normal = normal_df.to_numpy()\n",
        "anomaly_df.drop(\"target\", axis=1, errors=\"ignore\", inplace=True)\n",
        "anomaly = anomaly_df.to_numpy()\n",
        "\n",
        "X_train, X_test = train_test_split(normal, test_size=0.15, random_state=45, shuffle=True)\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}, anomaly shape: {anomaly.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "46d8c6d5",
      "metadata": {
        "id": "46d8c6d5"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.set_random_seed(1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "80bc66ff",
      "metadata": {
        "id": "80bc66ff"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, BatchNormalization\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b7907b67",
      "metadata": {
        "id": "b7907b67"
      },
      "outputs": [],
      "source": [
        "# Define the grid search parameters\n",
        "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8e4eed",
      "metadata": {
        "id": "7f8e4eed"
      },
      "source": [
        "# 2 methode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e779b8be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "e779b8be",
        "outputId": "db32c379-56bf-4e96-9a64-f8a2a781b6b3"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7844b54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7844b54",
        "outputId": "d70b46ec-bd57-4e9a-e2c3-dda5da094cf4"
      },
      "outputs": [],
      "source": [
        "unique_values = df['target'].unique()\n",
        "unique_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5070271",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "a5070271",
        "outputId": "5a448850-aeab-4c54-cceb-13518d2a0758"
      },
      "outputs": [],
      "source": [
        "value_counts = df['target'].value_counts()\n",
        "value_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wKbpqxQdp_Hv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "wKbpqxQdp_Hv",
        "outputId": "08197989-377a-4d3c-f000-d15c4a9ed6f3"
      },
      "outputs": [],
      "source": [
        "plt.bar(value_counts.index, value_counts.values)\n",
        "plt.xlabel(\"Target Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Target Values\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "14e0b5ef",
      "metadata": {
        "id": "14e0b5ef"
      },
      "outputs": [],
      "source": [
        "# Define a mapping dictionary\n",
        "mapping = {'Normal': 1, 'Anomaly': 0}\n",
        "\n",
        "# Apply the mapping to the 'target' column\n",
        "df['target_numeric'] = df['target'].map(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fc4bbed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "6fc4bbed",
        "outputId": "f9fc06ad-01ac-4aca-dd75-b6ff5516fc77"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "93d326db",
      "metadata": {
        "id": "93d326db"
      },
      "outputs": [],
      "source": [
        "df.drop('target', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f1cb65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "30f1cb65",
        "outputId": "463b3ec3-2893-4a09-cb61-aabd79dc92f5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed895f57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed895f57",
        "outputId": "7df84c67-c678-4491-f57f-abcd6a36a57d"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6d441b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec6d441b",
        "outputId": "00164829-d1a0-4737-8352-ca06c5e12022"
      },
      "outputs": [],
      "source": [
        "df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8009e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "9f8009e8",
        "outputId": "3b9ebbc4-6d5d-49f5-f79b-de165cd952c5"
      },
      "outputs": [],
      "source": [
        "abnormal = df[df['target_numeric'] ==0][:10]\n",
        "normal = df[df['target_numeric'] ==1][:10]\n",
        "\n",
        "fig = go.Figure()\n",
        "leg  = [False] * abnormal.shape[0]\n",
        "leg[0] = True\n",
        "\n",
        "for i in range(abnormal.shape[0]):\n",
        "    fig.add_trace(go.Scatter( x=np.arange(abnormal.shape[1]),y=abnormal.iloc[i,:],name='Abnormal ECG', mode='lines',  marker_color='rgba(255, 0, 0, 0.9)', showlegend= leg[i]))\n",
        "\n",
        "for j in range(normal.shape[0]):\n",
        "    fig.add_trace(go.Scatter( x=np.arange(normal.shape[1]),y=normal.iloc[j,:],name='Normal ECG',  mode='lines',  marker_color='rgba(0, 255, 0, 1)', showlegend= leg[j]))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_layout(xaxis_title=\"time\", yaxis_title=\"Signal\", title= {'text': 'Difference between different ECG', 'xanchor': 'center', 'yanchor': 'top', 'x':0.5} , bargap=0,)\n",
        "fig.update_traces(opacity=0.5)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1c0901e",
      "metadata": {
        "id": "d1c0901e"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dtip7fqWlQrR",
      "metadata": {
        "id": "dtip7fqWlQrR"
      },
      "source": [
        "# **Noise filtring**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "108a9cbc",
      "metadata": {
        "id": "108a9cbc"
      },
      "outputs": [],
      "source": [
        "ecg_data = df.iloc[:,:-1]\n",
        "labels = df.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "5725e428",
      "metadata": {
        "id": "5725e428"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "ecg_data = scaler.fit_transform(ecg_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40de5ace",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "40de5ace",
        "outputId": "c8ae7664-92fe-4afe-cb8c-6cb7856aadd5"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4f9161",
      "metadata": {
        "id": "9b4f9161"
      },
      "source": [
        "# Plot the graphs of unfiltered and filtered signals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L8efeU84sSY_",
      "metadata": {
        "id": "L8efeU84sSY_"
      },
      "source": [
        "**Plot original ECG signal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389e2b27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "389e2b27",
        "outputId": "ab4fcfd4-5647-4efa-b40e-226238d9f46d"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=np.arange(ecg_data.shape[0]), y=ecg_data[30], mode='lines', name='Original ECG signal'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TI8DNtjGsMiD",
      "metadata": {
        "id": "TI8DNtjGsMiD"
      },
      "source": [
        "**Filtering techniques**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "PQehEAXzr5VU",
      "metadata": {
        "id": "PQehEAXzr5VU"
      },
      "outputs": [],
      "source": [
        "# Median filtering\n",
        "ecg_medfilt = medfilt(ecg_data, kernel_size=3)\n",
        "\n",
        "# Low-pass filtering\n",
        "lowcut = 0.05\n",
        "highcut = 20.0\n",
        "nyquist = 0.5 * 360.0\n",
        "low = lowcut / nyquist\n",
        "high = highcut / nyquist\n",
        "b, a = butter(4, [low, high], btype='band')\n",
        "ecg_lowpass = filtfilt(b, a, ecg_data)\n",
        "\n",
        "# Wavelet filtering\n",
        "coeffs = pywt.wavedec(ecg_data, 'db4', level=1)\n",
        "threshold = np.std(coeffs[-1]) * np.sqrt(2*np.log(len(ecg_data)))\n",
        "coeffs[1:] = (pywt.threshold(i, value=threshold, mode='soft') for i in coeffs[1:])\n",
        "ecg_wavelet = pywt.waverec(coeffs, 'db4')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pNOwpN2IsH5T",
      "metadata": {
        "id": "pNOwpN2IsH5T"
      },
      "source": [
        "**Plot filtered ECG signals**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XiO-tybCr-qN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "XiO-tybCr-qN",
        "outputId": "56b01375-1e0b-4b70-8365-a81b04323c08"
      },
      "outputs": [],
      "source": [
        "fig.add_trace(go.Scatter(x=np.arange(ecg_medfilt.shape[0]), y=ecg_medfilt[30], mode='lines', name='Median filtered ECG signal'))\n",
        "fig.add_trace(go.Scatter(x=np.arange(ecg_lowpass.shape[0]), y=ecg_lowpass[30], mode='lines', name='Low-pass filtered ECG signal'))\n",
        "fig.add_trace(go.Scatter(x=np.arange(ecg_wavelet.shape[0]), y=ecg_wavelet[30], mode='lines', name='Wavelet filtered ECG signal'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66c38e2",
      "metadata": {
        "id": "d66c38e2"
      },
      "source": [
        "# Choosing the best filtering technique"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031472ef",
      "metadata": {
        "id": "031472ef"
      },
      "source": [
        "## Note :\n",
        "#### We will evaluate the filtering methods by calculating each algorithm’s mean squared error (MSE). The algorithm with the lowest MSE will be selected. To calculate MSE, both signals’ dataframe must have an equal number of rows. If the dataset is reduced after filtering, this step involves padding the data with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "bde887d3",
      "metadata": {
        "id": "bde887d3"
      },
      "outputs": [],
      "source": [
        "def pad_data(original_data,filtered_data):\n",
        "  diff = original_data.shape[1] - filtered_data.shape[1]\n",
        "\n",
        "  if diff > 0:\n",
        "\n",
        "      padding = np.zeros((filtered_data.shape[0], original_data.shape[1]))\n",
        "      padded_data = np.concatenate((filtered_data, padding))\n",
        "\n",
        "  elif diff < 0:\n",
        "      padded_data = filtered_data[:,:-abs(diff)]\n",
        "\n",
        "  elif diff == 0:\n",
        "      padded_data = filtered_data\n",
        "\n",
        "  return padded_data\n",
        "\n",
        "def mse(original_data, filtered_data):\n",
        "    filter_data = pad_data(original_data,filtered_data)\n",
        "    return np.mean((original_data - filter_data) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4P-uTa9FskdP",
      "metadata": {
        "id": "4P-uTa9FskdP"
      },
      "source": [
        "**Calculate the MSE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TlpRDjNSsgth",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlpRDjNSsgth",
        "outputId": "acb9a553-8e35-40ae-d336-b6924ab7925d"
      },
      "outputs": [],
      "source": [
        "mse_value_m = mse(ecg_data, ecg_medfilt)\n",
        "mse_value_l = mse(ecg_data, ecg_lowpass)\n",
        "mse_value_w = mse(ecg_data, ecg_wavelet)\n",
        "print(\"MSE value of Median Filtering:\", mse_value_m)\n",
        "print(\"MSE value of Low-pass Filtering:\", mse_value_l)\n",
        "print(\"MSE value of Wavelet Filtering:\", mse_value_w)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "9f006c64",
      "metadata": {
        "id": "9f006c64"
      },
      "source": [
        "Wavlet filtering is chosen based on visualisation and the error comparison,both."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a24c28",
      "metadata": {
        "id": "a4a24c28"
      },
      "source": [
        "# Splitting Data into Train & Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "965af345",
      "metadata": {
        "id": "965af345"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(ecg_wavelet, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e46c7c7",
      "metadata": {
        "id": "9e46c7c7"
      },
      "source": [
        "# Feature engineering: Choosing relevant features"
      ]
    },
    {
      "cell_type": "raw",
      "id": "4dbdee2d",
      "metadata": {
        "id": "4dbdee2d"
      },
      "source": [
        "In this context, the following features are being used:\n",
        "\n",
        " 1-T amplitude: The height of the T wave on the electrocardiogram (ECG) graph, which symbolizes the heart’s return to rest following a contraction\n",
        " 2-R amplitude: The height of the R wave on the ECG graph represents the heart muscle’s initial contraction.\n",
        " 3-RR interval: The time between two consecutive R waves on the ECG graph, representing the time between heartbeats.\n",
        "QRS duration: The time it takes for the QRS complex, representing the electrical signal traveling through the ventricles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "8cafe508",
      "metadata": {
        "id": "8cafe508"
      },
      "outputs": [],
      "source": [
        "features = []\n",
        "\n",
        "for i in range(X_train.shape[0]):\n",
        "    #Finding the R-peaks\n",
        "    r_peaks = scipy.signal.find_peaks(X_train[i])[0]\n",
        "\n",
        "    #Initialize lists to hold R-peak and T-peak amplitudes\n",
        "    r_amplitudes = []\n",
        "    t_amplitudes = []\n",
        "\n",
        "    # Iterate through R-peak locations to find corresponding T-peak amplitudes\n",
        "    for r_peak in r_peaks:\n",
        "        # Find the index of the T-peak (minimum value) in the interval from R-peak to R-peak + 200 samples\n",
        "        t_peak = np.argmin(X_train[i][r_peak:r_peak+200]) + r_peak\n",
        "        #Append the R-peak amplitude and T-peak amplitude to the lists\n",
        "        r_amplitudes.append(X_train[i][r_peak])\n",
        "        t_amplitudes.append(X_train[i][t_peak])\n",
        "\n",
        "    # extracting singular value metrics from the r_amplitudes\n",
        "    std_r_amp = np.std(r_amplitudes)\n",
        "    mean_r_amp = np.mean(r_amplitudes)\n",
        "    median_r_amp = np.median(r_amplitudes)\n",
        "    sum_r_amp = np.sum(r_amplitudes)\n",
        "    # extracting singular value metrics from the t_amplitudes\n",
        "    std_t_amp = np.std(t_amplitudes)\n",
        "    mean_t_amp = np.mean(t_amplitudes)\n",
        "    median_t_amp = np.median(t_amplitudes)\n",
        "    sum_t_amp = np.sum(t_amplitudes)\n",
        "\n",
        "    # Find the time between consecutive R-peaks\n",
        "    rr_intervals = np.diff(r_peaks)\n",
        "\n",
        "    # Calculate the time duration of the data collection\n",
        "    time_duration = (len(X_train[i]) - 1) / 1000 # assuming data is in ms\n",
        "\n",
        "    # Calculate the sampling rate\n",
        "    sampling_rate = len(X_train[i]) / time_duration\n",
        "\n",
        "    # Calculate heart rate\n",
        "    duration = len(X_train[i]) / sampling_rate\n",
        "    heart_rate = (len(r_peaks) / duration) * 60\n",
        "\n",
        "    # QRS duration\n",
        "    qrs_duration = []\n",
        "    for j in range(len(r_peaks)):\n",
        "        qrs_duration.append(r_peaks[j]-r_peaks[j-1])\n",
        "    # extracting singular value metrics from the qrs_durations\n",
        "    std_qrs = np.std(qrs_duration)\n",
        "    mean_qrs = np.mean(qrs_duration)\n",
        "    median_qrs = np.median(qrs_duration)\n",
        "    sum_qrs = np.sum(qrs_duration)\n",
        "\n",
        "    # Extracting the singular value metrics from the RR-interval\n",
        "    std_rr = np.std(rr_intervals)\n",
        "    mean_rr = np.mean(rr_intervals)\n",
        "    median_rr = np.median(rr_intervals)\n",
        "    sum_rr = np.sum(rr_intervals)\n",
        "\n",
        "    # Extracting the overall standard deviation\n",
        "    std = np.std(X_train[i])\n",
        "\n",
        "    # Extracting the overall mean\n",
        "    mean = np.mean(X_train[i])\n",
        "\n",
        "    # Appending the features to the list\n",
        "    features.append([mean, std, std_qrs, mean_qrs,median_qrs, sum_qrs, std_r_amp, mean_r_amp, median_r_amp, sum_r_amp, std_t_amp, mean_t_amp, median_t_amp, sum_t_amp, sum_rr, std_rr, mean_rr,median_rr, heart_rate])\n",
        "\n",
        "# Converting the list to a numpy array\n",
        "features = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929ca622",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "929ca622",
        "outputId": "e8e1c430-f0de-407b-ebe0-1d57bb38437d"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b51464",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02b51464",
        "outputId": "95577240-93cd-4fcc-dd6a-7c23397aaee8"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "34220053",
      "metadata": {
        "id": "34220053"
      },
      "outputs": [],
      "source": [
        "# Initializing an empty list to store the features\n",
        "X_test_fe = []\n",
        "\n",
        "# Extracting features for each sample\n",
        "for i in range(X_test.shape[0]):\n",
        "    # Finding the R-peaks\n",
        "    r_peaks = scipy.signal.find_peaks(X_test[i])[0]\n",
        "\n",
        "    # Initialize lists to hold R-peak and T-peak amplitudes\n",
        "    r_amplitudes = []\n",
        "    t_amplitudes = []\n",
        "\n",
        "    # Iterate through R-peak locations to find corresponding T-peak amplitudes\n",
        "    for r_peak in r_peaks:\n",
        "        # Find the index of the T-peak (minimum value) in the interval from R-peak to R-peak + 200 samples\n",
        "        t_peak = np.argmin(X_test[i][r_peak:r_peak+200]) + r_peak\n",
        "        # Append the R-peak amplitude and T-peak amplitude to the lists\n",
        "        r_amplitudes.append(X_test[i][r_peak])\n",
        "        t_amplitudes.append(X_test[i][t_peak])\n",
        "    #extracting singular value metrics from the r_amplitudes\n",
        "    std_r_amp = np.std(r_amplitudes)\n",
        "    mean_r_amp = np.mean(r_amplitudes)\n",
        "    median_r_amp = np.median(r_amplitudes)\n",
        "    sum_r_amp = np.sum(r_amplitudes)\n",
        "    #extracting singular value metrics from the t_amplitudes\n",
        "    std_t_amp = np.std(t_amplitudes)\n",
        "    mean_t_amp = np.mean(t_amplitudes)\n",
        "    median_t_amp = np.median(t_amplitudes)\n",
        "    sum_t_amp = np.sum(t_amplitudes)\n",
        "\n",
        "    # Find the time between consecutive R-peaks\n",
        "    rr_intervals = np.diff(r_peaks)\n",
        "\n",
        "    # Calculate the time duration of the data collection\n",
        "    time_duration = (len(X_test[i]) - 1) / 1000 # assuming data is in ms\n",
        "\n",
        "    # Calculate the sampling rate\n",
        "    sampling_rate = len(X_test[i]) / time_duration\n",
        "\n",
        "    # Calculate heart rate\n",
        "    duration = len(X_test[i]) / sampling_rate\n",
        "    heart_rate = (len(r_peaks) / duration) * 60\n",
        "\n",
        "    # QRS duration\n",
        "    qrs_duration = []\n",
        "    for j in range(len(r_peaks)):\n",
        "        qrs_duration.append(r_peaks[j]-r_peaks[j-1])\n",
        "    #extracting singular value metrics from the qrs_duartions\n",
        "    std_qrs = np.std(qrs_duration)\n",
        "    mean_qrs = np.mean(qrs_duration)\n",
        "    median_qrs = np.median(qrs_duration)\n",
        "    sum_qrs = np.sum(qrs_duration)\n",
        "\n",
        "    # Extracting the standard deviation of the RR-interval\n",
        "    std_rr = np.std(rr_intervals)\n",
        "    mean_rr = np.mean(rr_intervals)\n",
        "    median_rr = np.median(rr_intervals)\n",
        "    sum_rr = np.sum(rr_intervals)\n",
        "\n",
        "      # Extracting the standard deviation of the RR-interval\n",
        "    std = np.std(X_test[i])\n",
        "\n",
        "    # Extracting the mean of the RR-interval\n",
        "    mean = np.mean(X_test[i])\n",
        "\n",
        "    # Appending the features to the list\n",
        "    X_test_fe.append([mean, std,  std_qrs, mean_qrs,median_qrs, sum_qrs, std_r_amp, mean_r_amp, median_r_amp, sum_r_amp, std_t_amp, mean_t_amp, median_t_amp, sum_t_amp, sum_rr, std_rr, mean_rr,median_rr,heart_rate])\n",
        "\n",
        "# Converting the list to a numpy array\n",
        "X_test_fe = np.array(X_test_fe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b424a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28b424a8",
        "outputId": "2a086538-9f7b-4e8d-a70d-974b491a17d4"
      },
      "outputs": [],
      "source": [
        "X_test_fe.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a118ce8",
      "metadata": {
        "id": "0a118ce8"
      },
      "source": [
        "# Model Building and Training"
      ]
    },
    {
      "cell_type": "raw",
      "id": "51dae54d",
      "metadata": {
        "id": "51dae54d"
      },
      "source": [
        "We will now build a Recurrent Neural Network LSTM model. First, we will reshape the data to make it compatible with the model. Then, we will create an LSTM model with only 2 layers. Then, we will train it on the features extracted from the data. Finally, we will make the predictions on the validation/test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d45dd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "93d45dd6",
        "outputId": "c2dccc89-cac5-4d86-fa38-1c3d625e042d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Define the number of features in the train dataframe\n",
        "num_features = features.shape[1]\n",
        "\n",
        "# Reshape the features data to be in the right shape for LSTM input\n",
        "features = np.asarray(features).astype('float32')\n",
        "features = features.reshape(features.shape[0], features.shape[1], 1)\n",
        "\n",
        "X_test_fe = np.asarray(X_test_fe).astype('float32')\n",
        "X_test_fe = X_test_fe.reshape(X_test_fe.shape[0], X_test_fe.shape[1], 1)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(features.shape[1], 1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Time the training process\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, y_train, validation_data=(X_test_fe, y_test), epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = time.time() - start_time\n",
        "print(\"Training Time:\", training_time, \"seconds\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_test_fe)\n",
        "\n",
        "# Convert the predicted values to binary labels\n",
        "y_pred = [1 if p > 0.5 else 0 for p in y_pred]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3cae0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "4b3cae0e",
        "outputId": "3e1402cb-b066-4607-8f26-886bcd912e3f"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e152d536",
      "metadata": {
        "id": "e152d536"
      },
      "source": [
        "# Evaluation of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871dc404",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "871dc404",
        "outputId": "a7e8a3c2-79a5-4236-a075-540895043ce5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#calculate the AUC score\n",
        "auc = round(roc_auc_score(y_test, y_pred),2)\n",
        "\n",
        "#classification report provides all metrics e.g. precision, recall, etc.\n",
        "all_met = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: \", acc*100, \"%\")\n",
        "print(\" \\n\")\n",
        "print(\"AUC:\", auc)\n",
        "print(\" \\n\")\n",
        "print(\"Classification Report: \\n\", all_met)\n",
        "print(\" \\n\")\n",
        "\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "conf_mat_df = pd.DataFrame(conf_mat, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
        "\n",
        "fig = px.imshow(conf_mat_df, text_auto= True, color_continuous_scale='Blues')\n",
        "fig.update_xaxes(side='top', title_text='Predicted')\n",
        "fig.update_yaxes(title_text='Actual')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a25ba2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "c4a25ba2",
        "outputId": "a78fa2cf-72f5-45fc-f5a6-a188b9b5538f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Plot training and validation error\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter( y=history.history['loss'], mode='lines', name='Training'))\n",
        "fig.add_trace(go.Scatter( y=history.history['val_loss'], mode='lines', name='Validation'))\n",
        "fig.update_layout(xaxis_title=\"Epoch\", yaxis_title=\"Error\", title= {'text': 'Model Error', 'xanchor': 'center', 'yanchor': 'top', 'x':0.5} , bargap=0)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d91b8b9",
      "metadata": {
        "id": "5d91b8b9"
      },
      "source": [
        "# using bilstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "616f5eff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "616f5eff",
        "outputId": "185d4852-64fa-43fa-97c1-9f9b2f4c9442",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Dense, Bidirectional\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Define the number of features in the train dataframe\n",
        "num_features = features.shape[1]\n",
        "\n",
        "# Reshape the features data to be in the right shape for LSTM input\n",
        "features = np.asarray(features).astype('float32')\n",
        "features = features.reshape(features.shape[0], features.shape[1], 1)\n",
        "\n",
        "X_test_fe = np.asarray(X_test_fe).astype('float32')\n",
        "X_test_fe = X_test_fe.reshape(X_test_fe.shape[0], X_test_fe.shape[1], 1)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64), input_shape=(features.shape[1], 1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, y_train, validation_data=(X_test_fe, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = time.time() - start_time\n",
        "print(\"Training Time:\", training_time, \"seconds\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_test_fe)\n",
        "\n",
        "# Convert the predicted values to binary labels\n",
        "y_pred = [1 if p > 0.5 else 0 for p in y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f81aba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "10f81aba",
        "outputId": "d9726ab3-2709-4eb8-f10c-0accd4c4d8b6"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840cde94",
      "metadata": {
        "id": "840cde94"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab087c46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "ab087c46",
        "outputId": "3f1922fb-b632-4460-e66d-f5760828b427"
      },
      "outputs": [],
      "source": [
        "# calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#calculate the AUC score\n",
        "auc = round(roc_auc_score(y_test, y_pred),2)\n",
        "\n",
        "#classification report provides all metrics e.g. precision, recall, etc.\n",
        "all_met = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: \", acc*100, \"%\")\n",
        "print(\" \\n\")\n",
        "print(\"AUC:\", auc)\n",
        "print(\" \\n\")\n",
        "print(\"Classification Report: \\n\", all_met)\n",
        "print(\" \\n\")\n",
        "\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "conf_mat_df = pd.DataFrame(conf_mat, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
        "\n",
        "fig = px.imshow(conf_mat_df, text_auto= True, color_continuous_scale='Blues')\n",
        "fig.update_xaxes(side='top', title_text='Predicted')\n",
        "fig.update_yaxes(title_text='Actual')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ehl5JDzmvVAF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehl5JDzmvVAF",
        "outputId": "4ecb396f-f44e-41b0-ba75-abcd2d1dead7"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276e5546",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "276e5546",
        "outputId": "c29b05f3-8228-435a-8096-c2f4d9ee2bcb"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation error\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter( y=history.history['loss'], mode='lines', name='Training'))\n",
        "fig.add_trace(go.Scatter( y=history.history['val_loss'], mode='lines', name='Validation'))\n",
        "fig.update_layout(xaxis_title=\"Epoch\", yaxis_title=\"Error\", title= {'text': 'Model Error', 'xanchor': 'center', 'yanchor': 'top', 'x':0.5} , bargap=0)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa1178d",
      "metadata": {
        "id": "afa1178d"
      },
      "source": [
        "# stack lstm , GRU, ConvLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bea8ce5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5bea8ce5",
        "outputId": "ae7c9e25-24ce-432e-8dbb-83dcb253ef26",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM, Dense, Bidirectional, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Dropout, ConvLSTM2D, GRU\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Define the number of features in the train dataframe\n",
        "num_features = features.shape[1]\n",
        "\n",
        "# Reshape the features data to be in the right shape for LSTM input\n",
        "features = np.asarray(features).astype('float32')\n",
        "features = features.reshape(features.shape[0], features.shape[1], 1)\n",
        "\n",
        "X_test_fe = np.asarray(X_test_fe).astype('float32')\n",
        "X_test_fe = X_test_fe.reshape(X_test_fe.shape[0], X_test_fe.shape[1], 1)\n",
        "\n",
        "# Stacked LSTM\n",
        "stacked_lstm_model = Sequential()\n",
        "stacked_lstm_model.add(LSTM(64, return_sequences=True, input_shape=(features.shape[1], 1)))\n",
        "stacked_lstm_model.add(LSTM(64))\n",
        "stacked_lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# GRU\n",
        "gru_model = Sequential()\n",
        "gru_model.add(GRU(64, input_shape=(features.shape[1], 1)))\n",
        "gru_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# CNN-LSTM\n",
        "cnn_lstm_model = Sequential()\n",
        "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(features.shape[1], 1)))\n",
        "cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_lstm_model.add(LSTM(64))\n",
        "cnn_lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "# Compile the models\n",
        "stacked_lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the models\n",
        "print('stacked_lstm : ')\n",
        "stacked_lstm_history = stacked_lstm_model.fit(features, y_train, validation_data=(X_test_fe, y_test), epochs=50, batch_size=32)\n",
        "print('gru : ')\n",
        "gru_history = gru_model.fit(features, y_train, validation_data=(X_test_fe, y_test), epochs=50, batch_size=32)\n",
        "print('cnn_lstm : ')\n",
        "cnn_lstm_history = cnn_lstm_model.fit(features, y_train, validation_data=(X_test_fe, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot Stacked LSTM accuracy\n",
        "plt.plot(stacked_lstm_history.history['accuracy'], label='Stacked LSTM Training Accuracy', color='blue')\n",
        "plt.plot(stacked_lstm_history.history['val_accuracy'], label='Stacked LSTM Validation Accuracy', color='skyblue')\n",
        "plt.title('Stacked LSTM Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Stacked LSTM loss\n",
        "plt.plot(stacked_lstm_history.history['loss'], label='Stacked LSTM Training Loss', color='red')\n",
        "plt.plot(stacked_lstm_history.history['val_loss'], label='Stacked LSTM Validation Loss', color='orange')\n",
        "plt.title('Stacked LSTM Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot GRU accuracy\n",
        "plt.plot(gru_history.history['accuracy'], label='GRU Training Accuracy', color='green')\n",
        "plt.plot(gru_history.history['val_accuracy'], label='GRU Validation Accuracy', color='lightgreen')\n",
        "plt.title('GRU Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot GRU loss\n",
        "plt.plot(gru_history.history['loss'], label='GRU Training Loss', color='purple')\n",
        "plt.plot(gru_history.history['val_loss'], label='GRU Validation Loss', color='violet')\n",
        "plt.title('GRU Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot CNN-LSTM accuracy\n",
        "plt.plot(cnn_lstm_history.history['accuracy'], label='CNN-LSTM Training Accuracy', color='cyan')\n",
        "plt.plot(cnn_lstm_history.history['val_accuracy'], label='CNN-LSTM Validation Accuracy', color='lightblue')\n",
        "plt.title('CNN-LSTM Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot CNN-LSTM loss\n",
        "plt.plot(cnn_lstm_history.history['loss'], label='CNN-LSTM Training Loss', color='magenta')\n",
        "plt.plot(cnn_lstm_history.history['val_loss'], label='CNN-LSTM Validation Loss', color='pink')\n",
        "plt.title('CNN-LSTM Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_pred =stacked_lstm_model.predict(X_test_fe)\n",
        "y_pred_stack = [1 if p > 0.5 else 0 for p in y_pred_stack]\n",
        "\n",
        "y_pred =gru_model.predict(X_test_fe)\n",
        "y_pred_gru_model = [1 if p > 0.5 else 0 for p in y_pred_gru_model]\n",
        "\n",
        "y_pred =cnn_lstm_model.predict(X_test_fe)\n",
        "y_pred_cnn_lstm = [1 if p > 0.5 else 0 for p in y_pred_cnn_lstm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f34567e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "4f34567e",
        "outputId": "f7abac93-00f6-42b4-f5ec-ad317f49e910"
      },
      "outputs": [],
      "source": [
        "stacked_lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71850b40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "71850b40",
        "outputId": "df0fc519-3c0a-49d5-ee5d-d62fb509578f"
      },
      "outputs": [],
      "source": [
        "gru_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77631be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "77631be4",
        "outputId": "d1df6295-59ce-432d-91f8-496a20b6ea81"
      },
      "outputs": [],
      "source": [
        "cnn_lstm_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683f5ba6",
      "metadata": {
        "id": "683f5ba6"
      },
      "source": [
        "# Models evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0cb7a6",
      "metadata": {
        "id": "ed0cb7a6"
      },
      "source": [
        "# Stack Lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f1db94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "77f1db94",
        "outputId": "b8a6c3e3-d05a-4911-c66c-f7fa3eac20ea"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred_stakLstm = stacked_lstm_model.predict(X_test_fe)\n",
        "# Convert the predicted values to binary labels\n",
        "y_pred_stakLstm = [1 if p > 0.5 else 0 for p in y_pred_stakLstm]\n",
        "\n",
        "\n",
        "# calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred_stakLstm)\n",
        "\n",
        "#calculate the AUC score\n",
        "auc = round(roc_auc_score(y_test, y_pred_stakLstm),2)\n",
        "\n",
        "#classification report provides all metrics e.g. precision, recall, etc.\n",
        "all_met = classification_report(y_test, y_pred_stakLstm)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: \", acc*100, \"%\")\n",
        "print(\" \\n\")\n",
        "print(\"AUC:\", auc)\n",
        "print(\" \\n\")\n",
        "print(\"Classification Report: \\n\", all_met)\n",
        "print(\" \\n\")\n",
        "\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred_stakLstm)\n",
        "conf_mat_df = pd.DataFrame(conf_mat, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
        "\n",
        "fig = px.imshow(conf_mat_df, text_auto= True, color_continuous_scale='Blues')\n",
        "fig.update_xaxes(side='top', title_text='Predicted')\n",
        "fig.update_yaxes(title_text='Actual')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbc3be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "2bbc3be4",
        "outputId": "0efafec6-15b1-4070-c61f-f8190b353ed5"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation error\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter( y=stacked_lstm_history.history['loss'], mode='lines', name='Training'))\n",
        "fig.add_trace(go.Scatter( y=stacked_lstm_history.history['val_loss'], mode='lines', name='Validation'))\n",
        "fig.update_layout(xaxis_title=\"Epoch\", yaxis_title=\"Error\", title= {'text': 'Model Error', 'xanchor': 'center', 'yanchor': 'top', 'x':0.5} , bargap=0)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b2bed0",
      "metadata": {
        "id": "14b2bed0"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfc52f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cfc52f0",
        "outputId": "c2cbbb54-0ad7-4773-d4d6-c84d64ea7b14"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred_gru_history = gru_model.predict(X_test_fe)\n",
        "# Convert the predicted values to binary labels\n",
        "y_pred_gru_history = [1 if p > 0.5 else 0 for p in y_pred_gru_history]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4348707",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "f4348707",
        "outputId": "bc5b3fbe-e6a1-4aeb-cf14-b9ae58a0d398"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(y_test, y_pred_gru_history)\n",
        "\n",
        "#calculate the AUC score\n",
        "auc = round(roc_auc_score(y_test, y_pred_gru_history),2)\n",
        "\n",
        "#classification report provides all metrics e.g. precision, recall, etc.\n",
        "all_met = classification_report(y_test, y_pred_gru_history)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: \", acc*100, \"%\")\n",
        "print(\" \\n\")\n",
        "print(\"AUC:\", auc)\n",
        "print(\" \\n\")\n",
        "print(\"Classification Report: \\n\", all_met)\n",
        "print(\" \\n\")\n",
        "\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred_gru_history)\n",
        "conf_mat_df = pd.DataFrame(conf_mat, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
        "\n",
        "fig = px.imshow(conf_mat_df, text_auto= True, color_continuous_scale='Blues')\n",
        "fig.update_xaxes(side='top', title_text='Predicted')\n",
        "fig.update_yaxes(title_text='Actual')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e8ac57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "d3e8ac57",
        "outputId": "026bd388-1361-4170-a537-748162800b7d"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation error\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter( y=gru_history.history['loss'], mode='lines', name='Training'))\n",
        "fig.add_trace(go.Scatter( y=gru_history.history['val_loss'], mode='lines', name='Validation'))\n",
        "fig.update_layout(xaxis_title=\"Epoch\", yaxis_title=\"Error\", title= {'text': 'Model Error', 'xanchor': 'center', 'yanchor': 'top', 'x':0.5} , bargap=0)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec8bea3",
      "metadata": {
        "id": "5ec8bea3"
      },
      "source": [
        "# ConvLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7aad70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb7aad70",
        "outputId": "ca56fbf1-072b-4855-e3ba-41d570514098"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred_cnnlstm = cnn_lstm_model.predict(X_test_fe)\n",
        "# Convert the predicted values to binary labels\n",
        "y_pred_cnnlstm = [1 if p > 0.5 else 0 for p in y_pred_gru_history]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685c4514",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "685c4514",
        "outputId": "2a97398e-32be-4b12-a521-7412d0f1b3a3"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(y_test, y_pred_cnnlstm)\n",
        "\n",
        "#calculate the AUC score\n",
        "auc = round(roc_auc_score(y_test, y_pred_cnnlstm),2)\n",
        "\n",
        "#classification report provides all metrics e.g. precision, recall, etc.\n",
        "all_met = classification_report(y_test, y_pred_cnnlstm)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: \", acc*100, \"%\")\n",
        "print(\" \\n\")\n",
        "print(\"AUC:\", auc)\n",
        "print(\" \\n\")\n",
        "print(\"Classification Report: \\n\", all_met)\n",
        "print(\" \\n\")\n",
        "\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred_cnnlstm)\n",
        "conf_mat_df = pd.DataFrame(conf_mat, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
        "\n",
        "fig = px.imshow(conf_mat_df, text_auto= True, color_continuous_scale='Blues')\n",
        "fig.update_xaxes(side='top', title_text='Predicted')\n",
        "fig.update_yaxes(title_text='Actual')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea78a63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "dea78a63",
        "outputId": "94f1ba20-e28a-40fa-b26a-63c65a2b49e7"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation error\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter( y=cnn_lstm_history.history['loss'], mode='lines', name='Training'))\n",
        "fig.add_trace(go.Scatter( y=cnn_lstm_history.history['val_loss'], mode='lines', name='Validation'))\n",
        "fig.update_layout(xaxis_title=\"Epoch\", yaxis_title=\"Error\", title= {'text': 'Model Error', 'xanchor': 'center', 'yanchor': 'top', 'x':0.5} , bargap=0)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4148e5b4",
      "metadata": {
        "id": "4148e5b4"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "6a335fb4",
      "metadata": {
        "id": "6a335fb4"
      },
      "outputs": [],
      "source": [
        "raw_data = df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "2bb219b5",
      "metadata": {
        "id": "2bb219b5"
      },
      "outputs": [],
      "source": [
        "labels = raw_data[:, -1]\n",
        "\n",
        "# The other data points are the electrocadriogram data\n",
        "data = raw_data[:, 0:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "a56c2a4f",
      "metadata": {
        "id": "a56c2a4f"
      },
      "outputs": [],
      "source": [
        "# Wavelet filtering\n",
        "coeffs = pywt.wavedec(data, 'db4', level=1)\n",
        "threshold = np.std(coeffs[-1]) * np.sqrt(2*np.log(len(data)))\n",
        "coeffs[1:] = (pywt.threshold(i, value=threshold, mode='soft') for i in coeffs[1:])\n",
        "data = pywt.waverec(coeffs, 'db4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "6ff1b3ce",
      "metadata": {
        "id": "6ff1b3ce"
      },
      "outputs": [],
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=21\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53b8490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f53b8490",
        "outputId": "69deb2a4-8005-4dca-a68d-6ac58c8df089"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "0b1c3598",
      "metadata": {
        "id": "0b1c3598"
      },
      "outputs": [],
      "source": [
        "min_val = tf.reduce_min(train_data)\n",
        "max_val = tf.reduce_max(train_data)\n",
        "\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\n",
        "test_data = (test_data - min_val) / (max_val - min_val)\n",
        "\n",
        "train_data = tf.cast(train_data, tf.float32)\n",
        "test_data = tf.cast(test_data, tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "c9b238c6",
      "metadata": {
        "id": "c9b238c6"
      },
      "outputs": [],
      "source": [
        "train_labels = train_labels.astype(bool)\n",
        "test_labels = test_labels.astype(bool)\n",
        "\n",
        "normal_train_data = train_data[train_labels]\n",
        "normal_test_data = test_data[test_labels]\n",
        "\n",
        "anomalous_train_data = train_data[~train_labels]\n",
        "anomalous_test_data = test_data[~test_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5c1feb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "5e5c1feb",
        "outputId": "12e58fbe-b7ef-47ab-d31b-e711a5974025"
      },
      "outputs": [],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(188), normal_train_data[0])\n",
        "plt.title(\"A Normal ECG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5d5a6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "7d5d5a6f",
        "outputId": "05a266b1-aa5b-4b38-ac81-1f37e83aacd3"
      },
      "outputs": [],
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(188), anomalous_train_data[0])\n",
        "plt.title(\"An Anomalous ECG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "90f54575",
      "metadata": {
        "id": "90f54575"
      },
      "outputs": [],
      "source": [
        "class AnomalyDetector(Model):\n",
        "  def __init__(self):\n",
        "    super(AnomalyDetector, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Dense(40, activation=\"relu\"),\n",
        "      layers.Dense(20, activation=\"relu\"),\n",
        "      layers.Dense(10, activation=\"relu\")])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(20, activation=\"relu\"),\n",
        "      layers.Dense(40, activation=\"relu\"),\n",
        "      layers.Dense(188, activation=\"sigmoid\")])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = AnomalyDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "b54f1f3f",
      "metadata": {
        "id": "b54f1f3f"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bbf5823",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bbf5823",
        "outputId": "1992cae5-8826-4709-a402-243ad7430053"
      },
      "outputs": [],
      "source": [
        "history = autoencoder.fit(normal_train_data, normal_train_data,\n",
        "          epochs=80,\n",
        "          batch_size=512,\n",
        "          validation_data=(test_data, test_data),\n",
        "          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fff7569",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "5fff7569",
        "outputId": "bdc604fb-6b32-4120-ab51-d1e3be212244"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d334fe96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "d334fe96",
        "outputId": "e464906b-fabd-4693-d24a-24396bb1a3f1"
      },
      "outputs": [],
      "source": [
        "encoded_data = autoencoder.encoder(normal_test_data).numpy()\n",
        "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
        "\n",
        "plt.plot(normal_test_data[0], 'b')\n",
        "plt.plot(decoded_data[0], 'r')\n",
        "plt.fill_between(np.arange(188), decoded_data[0], normal_test_data[0], color='lightcoral')\n",
        "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1c0520",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "8a1c0520",
        "outputId": "16fdceff-ccbb-4db1-bd40-68b53476f41f"
      },
      "outputs": [],
      "source": [
        "encoded_data = autoencoder.encoder(anomalous_test_data).numpy()\n",
        "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
        "\n",
        "plt.plot(anomalous_test_data[0], 'b')\n",
        "plt.plot(decoded_data[0], 'r')\n",
        "plt.fill_between(np.arange(188), decoded_data[0], anomalous_test_data[0], color='lightcoral')\n",
        "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9c0c3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "8e9c0c3d",
        "outputId": "1ddeeaa0-cac4-4a28-ccd1-993098aff21b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "reconstructions = autoencoder.predict(normal_train_data)\n",
        "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
        "\n",
        "plt.hist(train_loss[None,:], bins=50)\n",
        "plt.xlabel(\"Train loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d57d57c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d57d57c",
        "outputId": "58f94f03-684e-4e00-d259-51f435cf0827"
      },
      "outputs": [],
      "source": [
        "threshold = np.mean(train_loss) + np.std(train_loss)\n",
        "print(\"Threshold: \", threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38529265",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "38529265",
        "outputId": "a120aab4-31eb-4225-83cd-ca57a8b57f72"
      },
      "outputs": [],
      "source": [
        "reconstructions = autoencoder.predict(anomalous_test_data)\n",
        "test_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n",
        "\n",
        "plt.hist(test_loss[None, :], bins=50)\n",
        "plt.xlabel(\"Test loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "10cb2207",
      "metadata": {
        "id": "10cb2207"
      },
      "outputs": [],
      "source": [
        "def predict(model, data, threshold):\n",
        "  reconstructions = model(data)\n",
        "  loss = tf.keras.losses.mae(reconstructions, data)\n",
        "  return tf.math.less(loss, threshold)\n",
        "\n",
        "def print_stats(predictions, labels):\n",
        "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
        "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
        "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb71472d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb71472d",
        "outputId": "a02f7ed0-0931-4ad7-8169-84d82cb85c74"
      },
      "outputs": [],
      "source": [
        "preds = predict(autoencoder, test_data, threshold)\n",
        "print_stats(preds, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d25031b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d25031b",
        "outputId": "08f96479-3343-4d88-8c0e-60503ec92c9e"
      },
      "outputs": [],
      "source": [
        "def print_stats(predictions, labels):\n",
        "    # Confusion Matrix\n",
        "    conf_mat = confusion_matrix(labels, predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_mat)\n",
        "\n",
        "    # AUC Calculation\n",
        "    auc_score = roc_auc_score(labels, predictions)\n",
        "    print(\"AUC Score:\", auc_score)\n",
        "\n",
        "    # Additional Metrics\n",
        "    print(\"Accuracy:\", accuracy_score(labels, predictions))\n",
        "    print(\"Precision:\", precision_score(labels, predictions))\n",
        "    print(\"Recall:\", recall_score(labels, predictions))\n",
        "    print(\"F1 Score:\", f1_score(labels, predictions))  # Calculate and print F1-score\n",
        "\n",
        "# Generate predictions\n",
        "preds = predict(autoencoder, test_data, threshold)\n",
        "\n",
        "# Print statistics\n",
        "print_stats(preds, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1574bc",
      "metadata": {
        "id": "dd1574bc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
